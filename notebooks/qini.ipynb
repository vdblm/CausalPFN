{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5466ccb2",
   "metadata": {},
   "source": [
    "# Evaluating Policy-making with CATE values\n",
    "\n",
    "This is a notebook that evaluates Qini metrics for different RCT datasets spanning causal benchmarks and Uplift marketting datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a608687",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "dataset_patterns = \"Hill (1),Hill (2),*(Sub)*\"  # ,*(Sub)*\n",
    "method_patterns = \"CausalPFN,T Learner,S Learner,X Learner,DA Learner,Forest DR Learner\"\n",
    "max_context_length = 50_000\n",
    "replace_runs = False  # whether to replace existing runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47ddd9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sys\n",
    "from utils import *\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "load_dotenv(override=True)\n",
    "\n",
    "dataset_patterns = dataset_patterns.split(\",\")\n",
    "method_patterns = method_patterns.split(\",\")\n",
    "\n",
    "warnings.filterwarnings('ignore') # ignore warnings\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "seed = 82718\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# make the qini directory if it doesn't exist\n",
    "qini_dir = os.path.join(os.environ[\"OUTPUT_DIR\"], \"qini\")\n",
    "os.makedirs(qini_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b99411",
   "metadata": {},
   "source": [
    "## Setup all the different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887a7f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "import re\n",
    "from benchmarks import LentaDataset, CriteoDataset, HillstromDataset, X5Dataset, MegafonDataset\n",
    "# this might take some time to run & load\n",
    "datasets = {}\n",
    "\n",
    "for dataset_pattern in dataset_patterns:\n",
    "    if check_match(\"Hill (1)\", dataset_pattern):\n",
    "        print(\"Hit Hill (1)!\")\n",
    "        datasets[\"Hill (1)\"] = HillstromDataset(\n",
    "            seed=seed,\n",
    "            n_folds=5,\n",
    "            outcome_col=\"visit\",\n",
    "            control_arm = \"No E-Mail\",\n",
    "            treatment_arm= \"Womens E-Mail\",\n",
    "        )\n",
    "    if check_match(\"Hill (2)\", dataset_pattern):\n",
    "        print(\"Hit Hill (2)!\")\n",
    "        datasets[\"Hill (2)\"] = HillstromDataset(\n",
    "            seed=seed,\n",
    "            n_folds=5,\n",
    "            outcome_col=\"visit\",\n",
    "            control_arm = \"No E-Mail\",\n",
    "            treatment_arm= \"Mens E-Mail\",\n",
    "        )\n",
    "    if check_match(\"Criteo RCT\", dataset_pattern):\n",
    "        print(\"Hit Criteo RCT!\")\n",
    "        datasets[\"Criteo RCT\"] = CriteoDataset(\n",
    "            seed=seed,\n",
    "            n_folds=5,\n",
    "            outcome_col=\"visit\",\n",
    "            treatment_col=\"treatment\",\n",
    "        )\n",
    "    if check_match(\"Lenta RCT\", dataset_pattern):\n",
    "        print(\"Hit Lenta RCT!\")\n",
    "        datasets[\"Lenta RCT\"] = LentaDataset(\n",
    "            seed=seed,\n",
    "            n_folds=5,\n",
    "        )\n",
    "    if check_match(\"X5 RCT\", dataset_pattern):\n",
    "        print(\"Hit X5 RCT!\")\n",
    "        datasets[\"X5 RCT\"] = X5Dataset(\n",
    "            seed=seed,\n",
    "            n_folds=5,\n",
    "        )\n",
    "    if check_match(\"Megafon RCT\", dataset_pattern):\n",
    "        print(\"Hit Megafon RCT!\")\n",
    "        datasets[\"Megafon RCT\"] = MegafonDataset(\n",
    "            seed=seed,\n",
    "            n_folds=5,\n",
    "        )\n",
    "    if check_match(\"Criteo RCT (Sub)\", dataset_pattern):\n",
    "        print(\"Hit Criteo RCT (Sub)!\")\n",
    "        datasets[\"Criteo RCT (Sub)\"] = CriteoDataset(\n",
    "            seed=seed,\n",
    "            n_folds=5,\n",
    "            outcome_col=\"visit\",\n",
    "            treatment_col=\"treatment\",\n",
    "            subsample_max_rows=50_000,\n",
    "        )\n",
    "    if check_match(\"X5 RCT (Sub)\", dataset_pattern):\n",
    "        print(\"Hit X5 RCT (Sub)!\")\n",
    "        datasets[\"X5 RCT (Sub)\"] = X5Dataset(\n",
    "            seed=seed,\n",
    "            n_folds=5,\n",
    "            subsample_max_rows=50_000,\n",
    "        )\n",
    "    if check_match(\"Lenta RCT (Sub)\", dataset_pattern):\n",
    "        print(\"Hit Lenta RCT (Sub)!\")\n",
    "        datasets[\"Lenta RCT (Sub)\"] = LentaDataset(\n",
    "            seed=seed,\n",
    "            n_folds=5,\n",
    "            subsample_max_rows=50_000,\n",
    "        )\n",
    "    if check_match(\"Megafon RCT (Sub)\", dataset_pattern):\n",
    "        print(\"Hit Megafon RCT (Sub)!\")\n",
    "        datasets[\"Megafon RCT (Sub)\"] = MegafonDataset(\n",
    "            seed=seed,\n",
    "            n_folds=5,\n",
    "            subsample_max_rows=50_000,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d55019",
   "metadata": {},
   "source": [
    "## Sanity Checks for the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7278a4ec",
   "metadata": {},
   "source": [
    "Print the standard mean difference (SMD) for each feature of each dataset and print the average of this value for each data suite. This evaluates whether or not the evaluation benchmarks contain RCT data or not, which is important for the Qini and Uplift curves to be valid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364034fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing if the data is RCT:\")\n",
    "print(\n",
    "    \"\\tAll the test splits should have small SMD and the train splits should also have small SMDs if they are also RCTs:\"\n",
    ")\n",
    "for data_suite_name, dataset in datasets.items():\n",
    "    print(f\"Data suite: {data_suite_name}\")\n",
    "    train_smd = []\n",
    "    test_smd = []\n",
    "    for qini_data in dataset:\n",
    "        for i in range(min(qini_data.X_train.shape[1], 10)):\n",
    "            # Compute standard mean difference for each feature\n",
    "            treatment_group = qini_data.X_train[qini_data.t_train == 1][:, i]\n",
    "            control_group = qini_data.X_train[qini_data.t_train == 0][:, i]\n",
    "            smd = (np.mean(treatment_group) - np.mean(control_group)) / (\n",
    "                0.5 * (np.std(treatment_group) + np.std(control_group) + 1e-3)\n",
    "            )\n",
    "            train_smd.append(abs(smd))\n",
    "\n",
    "            treatment_group = qini_data.X_test[qini_data.t_test == 1][:, i]\n",
    "            control_group = qini_data.X_test[qini_data.t_test == 0][:, i]\n",
    "            smd = (np.mean(treatment_group) - np.mean(control_group)) / (\n",
    "                0.5 * (np.std(treatment_group) + np.std(control_group) + 1e-3)\n",
    "            )\n",
    "            test_smd.append(abs(smd))\n",
    "    print(f\">>>\\tTest SMD: {np.mean(test_smd):.4f} ± {np.std(test_smd):.4f}\")\n",
    "    print(f\">>>\\tTrain SMD: {np.mean(train_smd):.4f} ± {np.std(train_smd):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9304fad",
   "metadata": {},
   "source": [
    "## Run our estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f7cfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.causalpfn import CATEEstimator\n",
    "from benchmarks.base import Qini_Dataset\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "pbar = tqdm(\n",
    "    total=sum([len(dataset) for dataset in datasets.values()]),\n",
    "    desc=\"CausalPFN\",\n",
    ")\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    pbar.set_postfix(dataset=dataset_name)\n",
    "    for i in range(len(dataset)):\n",
    "        # dataset_name: str, method_name: str, all_method_patterns: list, all_datasets_patterns: list, idx: int, artifact_dir: str, replace: bool = False\n",
    "        with result_saver(\n",
    "            dataset_name=dataset_name,\n",
    "            method_name=\"CausalPFN\",\n",
    "            all_method_patterns=method_patterns,\n",
    "            all_datasets_patterns=dataset_patterns,\n",
    "            idx=i,\n",
    "            artifact_dir=qini_dir,\n",
    "            replace=replace_runs,\n",
    "        ) as result:\n",
    "            if result is not None:\n",
    "\n",
    "                qini_data: Qini_Dataset = dataset[i]\n",
    "                time_start = time.time()\n",
    "                cate_estimator_tabdpt = CATEEstimator(\n",
    "                    device=device,\n",
    "                    max_context_length=max_context_length,\n",
    "                )\n",
    "\n",
    "                cate_estimator_tabdpt.fit(X=qini_data.X_train, y=qini_data.y_train, t=qini_data.t_train)\n",
    "                estimated_tau = cate_estimator_tabdpt.estimate_cate(X=qini_data.X_test)\n",
    "                time_spent = time.time() - time_start\n",
    "                result[\"estimated_effect\"] = estimated_tau\n",
    "                result[\"t\"] = qini_data.t_test\n",
    "                result[\"y\"] = qini_data.y_test\n",
    "                result[\"time_spent\"] = time_spent / ((len(qini_data.X_test) + len(qini_data.X_train)) / 1000)\n",
    "\n",
    "            pbar.update(1)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a90f7f0",
   "metadata": {},
   "source": [
    "## Run the Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da19bd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baselines (Base)\n",
    "from benchmarks.baselines import BaselineModel\n",
    "\n",
    "# Baselines (EconML)\n",
    "from benchmarks.baselines import (\n",
    "    TLearnerBaseline,\n",
    "    SLearnerBaseline,\n",
    "    XLearnerBaseline,\n",
    "    DALearnerBaseline,\n",
    "    XLearnerBaseline,\n",
    "    ForestDRLearnerBaseline,\n",
    ")\n",
    "\n",
    "\n",
    "baselines = {\n",
    "    \"T Learner\": TLearnerBaseline(hpo=False),\n",
    "    \"S Learner\": SLearnerBaseline(hpo=False),\n",
    "    \"X Learner\": XLearnerBaseline(hpo=False),\n",
    "    \"DA Learner\": DALearnerBaseline(hpo=False),\n",
    "    \"Forest DR Learner\": ForestDRLearnerBaseline(hpo=False),\n",
    "}\n",
    "\n",
    "\n",
    "pbar = tqdm(\n",
    "    total=sum([len(dataset) * len(baselines) for dataset in datasets.values()]),\n",
    "    desc=\"Baselines\",\n",
    ")\n",
    "\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    for baseline_name, baseline in baselines.items():\n",
    "        pbar.set_postfix(dataset=dataset_name, baseline=baseline_name)\n",
    "        for i in range(len(dataset)):\n",
    "            with result_saver(\n",
    "                dataset_name=dataset_name,\n",
    "                method_name=baseline_name,\n",
    "                all_method_patterns=method_patterns,\n",
    "                all_datasets_patterns=dataset_patterns,\n",
    "                idx=i,\n",
    "                artifact_dir=qini_dir,\n",
    "                replace=replace_runs,\n",
    "            ) as result:\n",
    "                if result is not None:\n",
    "                    baseline: BaselineModel\n",
    "\n",
    "                    qini_data: Qini_Dataset = dataset[i]\n",
    "                    time_start = time.time()\n",
    "                    estimated_tau = baseline.estimate_cate(\n",
    "                        X_train=qini_data.X_train,\n",
    "                        t_train=qini_data.t_train,\n",
    "                        y_train=qini_data.y_train,\n",
    "                        X_test=qini_data.X_test,\n",
    "                    )\n",
    "                    time_spent = time.time() - time_start\n",
    "                    result[\"estimated_effect\"] = estimated_tau\n",
    "                    result[\"t\"] = qini_data.t_test\n",
    "                    result[\"y\"] = qini_data.y_test\n",
    "                    result[\"time_spent\"] = time_spent / ((len(qini_data.X_test) + len(qini_data.X_train)) / 1000)\n",
    "\n",
    "                pbar.update(1)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837f9130",
   "metadata": {},
   "source": [
    "## Compute Qini Curves & Scores \n",
    "\n",
    "To compute the Qini curves, we concatenate all of the outcomes, treatments, and estimated effects for each dataset into a single list, sort the estimated effects and compute the qini curve and scores. We do that across all datasets and baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848530cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.causalpfn.evaluation import get_qini_curve\n",
    "import pandas as pd\n",
    "\n",
    "methods_to_show = [\"CausalPFN\"] + list(baselines.keys())\n",
    "methods_to_show = [method for method in methods_to_show if any([check_match(method, m) for m in method_patterns])]\n",
    "datasets_to_show = list(datasets.keys())\n",
    "datasets_to_show = [dataset for dataset in datasets_to_show if any([check_match(dataset, d) for d in dataset_patterns])]\n",
    "qini_df = pd.DataFrame(columns=datasets_to_show)\n",
    "qini_curves = {}\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    qini_curves[dataset_name] = {}\n",
    "    dset_result = load_all_results(dataset_name, qini_dir)\n",
    "    for method in methods_to_show:\n",
    "        all_rows = dset_result[method]\n",
    "        num_realizations = len(all_rows[\"t\"])\n",
    "        all_t = []\n",
    "        all_y = []\n",
    "        all_tau = []\n",
    "        for fold_idx in range(num_realizations):\n",
    "            all_t.append(all_rows[\"t\"][fold_idx])\n",
    "            all_y.append(all_rows[\"y\"][fold_idx])\n",
    "            all_tau.append(all_rows[\"estimated_effect\"][fold_idx])\n",
    "        all_t = np.concatenate(all_t)\n",
    "        all_y = np.concatenate(all_y)\n",
    "        all_tau = np.concatenate(all_tau)\n",
    "        qini_curves[dataset_name][method], qini_score = get_qini_curve(\n",
    "            rct_treatments=all_t,\n",
    "            rct_outcomes=all_y,\n",
    "            estimated_cate=all_tau,\n",
    "            normalize=True,\n",
    "        )\n",
    "        qini_df.loc[method, dataset_name] = qini_score\n",
    "# devide every value by the maximum of the column\n",
    "# (uncomment to normalize)\n",
    "qini_df = qini_df / qini_df.max()\n",
    "# add an average of each row as the last column\n",
    "qini_df[\"Average\"] = qini_df.mean(axis=1)\n",
    "# sort the dataframe by the average column\n",
    "qini_df = qini_df.sort_values(by=\"Average\", ascending=False)\n",
    "qini_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a891317",
   "metadata": {},
   "source": [
    "Visualize the actual curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e588e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for dset in datasets_to_show:\n",
    "    for method in methods_to_show:\n",
    "        curve = qini_curves[dset][method]\n",
    "        plt.plot(np.linspace(0, 1, len(curve)), curve, label=method)\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"black\", label=\"Random\")\n",
    "    plt.title(f\"Qini Curve for {dset}\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Fraction of Population\")\n",
    "    plt.ylabel(\"Qini\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35239cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tracee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
