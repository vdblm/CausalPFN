{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a098247e",
   "metadata": {},
   "source": [
    "## Run the following setup code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad0a72fc",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "dataset_patterns = (\n",
    "    \"IHDP,ACIC 2016,RealCause Lalonde CPS,RealCause Lalonde PSID\"  # all of the datasets that are being run\n",
    ")\n",
    "method_patterns = \"*\"  # all of the methods to run\n",
    "index = None\n",
    "replace_runs = False  # whether to replace existing runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb41ba14",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = int(index) if index is not None else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718bed5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sys\n",
    "from utils import *\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "load_dotenv(override=True)\n",
    "\n",
    "warnings.filterwarnings('ignore') # ignore warnings\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dataset_patterns = dataset_patterns.split(\",\")\n",
    "method_patterns = method_patterns.split(\",\")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "seed = 82718\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8a35d5",
   "metadata": {},
   "source": [
    "Load all the datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47de04be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets and required functions\n",
    "%autoreload 2\n",
    "from benchmarks import IHDPDataset, ACIC2016Dataset\n",
    "from benchmarks import RealCauseLalondeCPSDataset, RealCauseLalondePSIDDataset\n",
    "\n",
    "# Store all the results for all the datasets\n",
    "\n",
    "IHDP = IHDPDataset()\n",
    "ACIC2016 = ACIC2016Dataset()\n",
    "RealCauseLalondeCPS = RealCauseLalondeCPSDataset()\n",
    "RealCauseLalondePSID = RealCauseLalondePSIDDataset()\n",
    "\n",
    "datasets = {\n",
    "    \"IHDP\": IHDP if index is None else [IHDP[index%len(IHDP)]],\n",
    "    \"ACIC 2016\": ACIC2016 if index is None else [ACIC2016[index%len(ACIC2016)]],\n",
    "    \"RealCause Lalonde CPS\": RealCauseLalondeCPS if index is None else [RealCauseLalondeCPS[index%len(RealCauseLalondeCPS)]],\n",
    "    \"RealCause Lalonde PSID\": RealCauseLalondePSID if index is None else [RealCauseLalondePSID[index%len(RealCauseLalondePSID)]],\n",
    "}\n",
    "\n",
    "causal_effect_path = os.path.join(os.environ[\"OUTPUT_DIR\"], \"causal_effect/\")\n",
    "os.makedirs(causal_effect_path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb959b16",
   "metadata": {},
   "source": [
    "## CausalPFN - TabDPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97a715c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.causalpfn import CATEEstimator, calculate_pehe, ATEEstimator\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "pbar = tqdm(\n",
    "    total=sum([len(dataset) for dataset in datasets.values()]),\n",
    "    desc=\"CausalPFN\",\n",
    ")\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    pbar.set_postfix(dataset=dataset_name)\n",
    "    for i in range(len(dataset)):\n",
    "        # dataset_name: str, method_name: str, all_method_patterns: list, all_datasets_patterns: list, idx: int, artifact_dir: str, replace: bool = False\n",
    "        with result_saver(\n",
    "            dataset_name=dataset_name,\n",
    "            method_name=\"CausalPFN\",\n",
    "            all_method_patterns=method_patterns,\n",
    "            all_datasets_patterns=dataset_patterns,\n",
    "            idx=i if index is None else index,\n",
    "            artifact_dir=causal_effect_path,\n",
    "            replace=replace_runs,\n",
    "        ) as result:\n",
    "            if result is not None:\n",
    "                cate_dset, ate_dset = dataset[i]\n",
    "\n",
    "                # CATE\n",
    "                time_start = time.time()\n",
    "                cate_estimator = CATEEstimator(\n",
    "                    device=device,\n",
    "                )\n",
    "                cate_estimator.fit(cate_dset.X_train, cate_dset.t_train, cate_dset.y_train)\n",
    "                estimated_cate = cate_estimator.estimate_cate(X=cate_dset.X_test)\n",
    "                time_spent = time.time() - time_start\n",
    "                pehe = calculate_pehe(cate_pred=estimated_cate, cate_true=cate_dset.true_cate)\n",
    "                result[\"pehe\"] = pehe\n",
    "                result[\"time_cate\"] = time_spent / (len(cate_dset.X_test) + len(cate_dset.X_train)) * 1000\n",
    "\n",
    "                # ATE\n",
    "                time_start = time.time()\n",
    "                ate_estimator = ATEEstimator(\n",
    "                    device=device,\n",
    "                )\n",
    "                ate_estimator.fit(ate_dset.X, ate_dset.t, ate_dset.y)\n",
    "                estimated_ate = ate_estimator.estimate_ate()\n",
    "                time_spent = time.time() - time_start\n",
    "                result[\"ate_rel_err\"] = abs(estimated_ate - ate_dset.true_ate) / abs(ate_dset.true_ate)\n",
    "                result[\"time_ate\"] = time_spent / len(ate_dset.X) * 1000\n",
    "\n",
    "            pbar.update(1)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f11a696",
   "metadata": {},
   "source": [
    "## Baselines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3423432e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baselines (Base)\n",
    "from benchmarks.baselines import BaselineModel\n",
    "\n",
    "# Baselines (EconML)\n",
    "from benchmarks.baselines import (\n",
    "    ForestDMLBaseline,\n",
    "    TLearnerBaseline,\n",
    "    SLearnerBaseline,\n",
    "    XLearnerBaseline,\n",
    "    DALearnerBaseline,\n",
    "    XLearnerBaseline,\n",
    "    ForestDRLearnerBaseline,\n",
    ")\n",
    "\n",
    "# Baselines (CATE Net)\n",
    "from benchmarks.baselines import TarNetBaseline, DragonNetBaseline, RANetBaseline\n",
    "\n",
    "# GRF & BART & IPW\n",
    "from benchmarks.baselines import GRFBaseline, BartBaseline, IPWBaseline\n",
    "\n",
    "\n",
    "baselines = {\n",
    "    \"T Learner (no HPO)\": TLearnerBaseline(hpo=False),\n",
    "    \"T Learner (HPO)\": TLearnerBaseline(hpo=True),\n",
    "    \"S Learner (no HPO)\": SLearnerBaseline(hpo=False),\n",
    "    \"S Learner (HPO)\": SLearnerBaseline(hpo=True),\n",
    "    \"X Learner (no HPO)\": XLearnerBaseline(hpo=False),\n",
    "    \"X Learner (HPO)\": XLearnerBaseline(hpo=True),\n",
    "    \"DA Learner (no HPO)\": DALearnerBaseline(hpo=False),\n",
    "    \"DA Learner (HPO)\": DALearnerBaseline(hpo=True),\n",
    "    \"Forest DR Learner (no HPO)\": ForestDRLearnerBaseline(hpo=False),\n",
    "    \"Forest DR Learner (HPO)\": ForestDRLearnerBaseline(hpo=True),\n",
    "    \"Forest DML (no HPO)\": ForestDMLBaseline(hpo=False),\n",
    "    \"Forest DML (HPO)\": ForestDMLBaseline(hpo=True),\n",
    "    \"DragonNet (no HPO)\": DragonNetBaseline(hpo=False),\n",
    "    \"DragonNet (HPO)\": DragonNetBaseline(hpo=True),\n",
    "    \"TarNet (no HPO)\": TarNetBaseline(hpo=False),\n",
    "    \"TarNet (HPO)\": TarNetBaseline(hpo=True),\n",
    "    \"RA Net (no HPO)\": RANetBaseline(hpo=False),\n",
    "    \"RA Net (HPO)\": RANetBaseline(hpo=True),\n",
    "    \"GRF (no HPO)\": GRFBaseline(hpo=False),\n",
    "    \"GRF (HPO)\": GRFBaseline(hpo=True),\n",
    "    \"BART\": BartBaseline(hpo=False),\n",
    "    \"IPW (no HPO)\": IPWBaseline(hpo=False),\n",
    "    \"IPW (HPO)\": IPWBaseline(hpo=True),\n",
    "}\n",
    "\n",
    "\n",
    "pbar = tqdm(\n",
    "    total=sum([len(dataset) * len(baselines) for dataset in datasets.values()]),\n",
    "    desc=\"Baselines\",\n",
    ")\n",
    "\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    for baseline_name, baseline in baselines.items():\n",
    "        pbar.set_postfix(dataset=dataset_name, baseline=baseline_name)\n",
    "        for i in range(len(dataset)):\n",
    "            with result_saver(\n",
    "                dataset_name=dataset_name,\n",
    "                method_name=baseline_name,\n",
    "                all_method_patterns=method_patterns,\n",
    "                all_datasets_patterns=dataset_patterns,\n",
    "                idx=i if index is None else index,\n",
    "                artifact_dir=causal_effect_path,\n",
    "                replace=replace_runs,\n",
    "            ) as result:\n",
    "                if result is not None:\n",
    "                    baseline: BaselineModel\n",
    "                    cate_dset, ate_dset = dataset[i]\n",
    "\n",
    "                    # CATE\n",
    "                    start_time = time.time()\n",
    "                    cate_pred = baseline.estimate_cate(\n",
    "                        X_train=cate_dset.X_train,\n",
    "                        t_train=cate_dset.t_train,\n",
    "                        y_train=cate_dset.y_train,\n",
    "                        X_test=cate_dset.X_test,\n",
    "                    )\n",
    "                    time_spent = time.time() - start_time\n",
    "                    pehe = calculate_pehe(cate_true=cate_dset.true_cate, cate_pred=cate_pred)\n",
    "                    result[\"pehe\"] = pehe\n",
    "                    result[\"time_cate\"] = time_spent / (len(cate_dset.X_test) + len(cate_dset.X_train)) * 1000\n",
    "\n",
    "                    # ATE\n",
    "                    start_time = time.time()\n",
    "                    ate_pred = baseline.estimate_ate(\n",
    "                        X=ate_dset.X,\n",
    "                        t=ate_dset.t,\n",
    "                        y=ate_dset.y,\n",
    "                    )\n",
    "                    time_spent = time.time() - start_time\n",
    "                    result[\"ate_rel_err\"] = abs(ate_pred - ate_dset.true_ate) / abs(ate_dset.true_ate)\n",
    "                    result[\"time_ate\"] = time_spent / len(ate_dset.X) * 1000\n",
    "                pbar.update(1)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1a99ed",
   "metadata": {},
   "source": [
    "## Parse and visualize all of the results\n",
    "\n",
    "Once done, parse all of the results that were stored with the following code into a dataframe. This dataframe will contain different rows for each causal task and columns for the dataset, fold, method, and metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1aa354b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "methods_to_show = [\"CausalPFN\"] + list(baselines.keys())\n",
    "methods_to_show = [method for method in methods_to_show if any([check_match(method, m) for m in method_patterns])]\n",
    "results_df = pd.DataFrame(columns=[\"dataset\", \"method\", \"pehe\", \"ate_rel_err\", \"time_cate\", \"time_ate\", \"realization\"])\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    dset_result = load_all_results(dataset_name, causal_effect_path)\n",
    "    for method in methods_to_show:\n",
    "        all_rows = dset_result[method]\n",
    "        num_realizations = len(all_rows[\"pehe\"])\n",
    "        for fold_idx in range(num_realizations):\n",
    "            pehe = float(all_rows[\"pehe\"][fold_idx])\n",
    "            ate_rel_err = float(all_rows[\"ate_rel_err\"][fold_idx])\n",
    "            time_cate = float(all_rows[\"time_cate\"][fold_idx])\n",
    "            time_ate = float(all_rows[\"time_ate\"][fold_idx])\n",
    "            new_row = dict(\n",
    "                dataset=dataset_name,\n",
    "                method=method,\n",
    "                cate_pehe=pehe,\n",
    "                ate_rel_err=ate_rel_err,\n",
    "                ate_time=time_cate,\n",
    "                cate_time=time_ate,\n",
    "                realization=fold_idx,\n",
    "            )\n",
    "            results_df = pd.concat([results_df, pd.DataFrame(new_row, index=[0])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b137175e",
   "metadata": {},
   "source": [
    "Once done, you can load the results below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc6086ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize all of the ATE and CATE estimation times by averaging over realizations\n",
    "time_spent_df = (\n",
    "    results_df.pivot_table(\n",
    "        index=\"method\",  # rows: one per method\n",
    "        columns=\"dataset\",  # multi‐columns: first level will be dataset\n",
    "        values=[\"ate_time\", \"cate_time\"],  # the values to aggregate\n",
    "        aggfunc=\"mean\",  # take the mean over realizations\n",
    "    )\n",
    "    .swaplevel(0, 1, axis=1)\n",
    "    .sort_index(axis=1, level=0)\n",
    ")\n",
    "\n",
    "# Compute mean and standard error for ATE and CATE metrics\n",
    "metrics = [\"cate_pehe\", \"ate_rel_err\"]\n",
    "grp = results_df.groupby([\"method\", \"dataset\"])[metrics].agg([\"mean\", \"sem\"])  # MultiIndex cols: (metric, agg)\n",
    "methods = grp.index.levels[0]\n",
    "datasets_index = grp.index.levels[1]\n",
    "data = {}\n",
    "for ds in datasets_index:\n",
    "    for m in metrics:\n",
    "        means = grp[(m, \"mean\")].xs(ds, level=\"dataset\")\n",
    "        sems = grp[(m, \"sem\")].xs(ds, level=\"dataset\")\n",
    "        # combine into \"xx.xx ± yy.yy\" strings\n",
    "        data[(ds, m)] = means.combine(sems, lambda mu, se: f\"{mu:.2f} ± {se:.2f}\")\n",
    "causal_effect_errors = pd.DataFrame(data, index=methods)\n",
    "causal_effect_errors.columns = pd.MultiIndex.from_tuples(causal_effect_errors.columns, names=[\"dataset\", \"metric\"])\n",
    "causal_effect_errors = causal_effect_errors.sort_index(axis=1, level=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5401a1ce",
   "metadata": {},
   "source": [
    "Compute total performance for CATE (rank of PEHE), and ATE (average relative error) for each method and all causal tasks (across different realizations of each method). Then visualize all of the error rates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763e04d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "ate_errors = defaultdict(list)\n",
    "cate_pehes = defaultdict(list)\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    dset_result = load_all_results(dataset_name, causal_effect_path)\n",
    "    for method in methods_to_show:\n",
    "        all_rows = dset_result[method]\n",
    "        num_realizations = len(all_rows[\"pehe\"])\n",
    "        for fold_idx in range(num_realizations):\n",
    "            pehe = float(all_rows[\"pehe\"][fold_idx])\n",
    "            ate_rel_err = float(all_rows[\"ate_rel_err\"][fold_idx])\n",
    "            ate_errors[method].append(ate_rel_err)\n",
    "            cate_pehes[method].append(pehe)\n",
    "\n",
    "\n",
    "def get_ranks(res: dict):\n",
    "    ranks = {}\n",
    "    ranks_ste = {}\n",
    "    for method in methods_to_show:\n",
    "        all_len = len(res[method])\n",
    "        all_ranks = []\n",
    "        for idx in range(all_len):\n",
    "            rank = 0\n",
    "            for other_methods in methods_to_show:\n",
    "                our_res = res[method][idx]\n",
    "                other_res = res[other_methods][idx]\n",
    "                rank += our_res >= other_res\n",
    "            all_ranks.append(rank)\n",
    "        ranks[method] = sum(all_ranks) / all_len\n",
    "        ranks_ste[method] = np.std(all_ranks) / np.sqrt(all_len)\n",
    "    return ranks, ranks_ste\n",
    "\n",
    "\n",
    "cate_ranks, cate_ranks_ste = get_ranks(cate_pehes)\n",
    "ate_ranks, ate_ranks_ste = get_ranks(ate_errors)\n",
    "\n",
    "# add a multicolumn to causal_effect_errors called \"overall\"\n",
    "causal_effect_errors[(\"overall\", \"cate_rank ± ste\")] = pd.Series(\n",
    "    {method: f\"{cate_ranks[method]:.2f} ± {cate_ranks_ste[method]:.2f}\" for method in methods_to_show}\n",
    ")\n",
    "causal_effect_errors[(\"overall\", \"ate_rank ± ste\")] = pd.Series(\n",
    "    {method: f\"{ate_ranks[method]:.2f} ± {ate_ranks_ste[method]:.2f}\" for method in methods_to_show}\n",
    ")\n",
    "\n",
    "causal_effect_errors[(\"overall\", \"rank\")] = (pd.Series(cate_ranks) + pd.Series(ate_ranks)) / 2\n",
    "# sort rows by rank\n",
    "causal_effect_errors = causal_effect_errors.sort_values(by=(\"overall\", \"rank\"))\n",
    "causal_effect_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c39b26",
   "metadata": {},
   "source": [
    "Compute average time for each method and add that to the dataframe of times and sort according to that average.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7318cc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "ate_times = defaultdict(list)\n",
    "cate_times = defaultdict(list)\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    dset_result = load_all_results(dataset_name, causal_effect_path)\n",
    "    for method in methods_to_show:\n",
    "        all_rows = dset_result[method]\n",
    "        num_realizations = len(all_rows[\"pehe\"])\n",
    "        for fold_idx in range(num_realizations):\n",
    "            cate_time = float(all_rows[\"time_cate\"][fold_idx])\n",
    "            ate_time = float(all_rows[\"time_ate\"][fold_idx])\n",
    "            ate_times[method].append(ate_time)\n",
    "            cate_times[method].append(cate_time)\n",
    "med_ate_times = {}\n",
    "med_cate_times = {}\n",
    "for method in methods_to_show:\n",
    "    med_ate_times[method] = np.median([ate_times[method][i] for i in range(len(ate_times[method]))])\n",
    "    med_cate_times[method] = np.median([cate_times[method][i] for i in range(len(cate_times[method]))])\n",
    "\n",
    "# add a multicolumn to causal_effect_errors called \"overall\"\n",
    "time_spent_df[(\"overall\", \"ate_time\")] = pd.Series(med_ate_times)\n",
    "time_spent_df[(\"overall\", \"cate_time\")] = pd.Series(med_cate_times)\n",
    "# sort rows by rank\n",
    "time_spent_df = time_spent_df.sort_values(by=(\"overall\", \"cate_time\"))\n",
    "time_spent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bcc9a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
